{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "040b5db5",
   "metadata": {},
   "source": [
    "# Domain Decoder: A Tool to Identify Fluoroquinolone Drug Binding Sites\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907c390e",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64269696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8d04c4",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5a6d324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Head:\n",
      "     -0.202641  -0.44152558  0.7237159  -0.051630057  -0.1796543  -0.17303413  \\\n",
      "AAA                                                                             \n",
      "ALA  -0.487939    -0.286591   0.723286      0.018458   -0.026013    -0.185017   \n",
      "LLL  -0.209581    -0.049195   0.687492      0.043283   -0.085327    -0.166621   \n",
      "LAA  -0.449757    -0.301040   0.650756     -0.038405   -0.143120    -0.177706   \n",
      "AAL  -0.322143    -0.279880   0.750441      0.031342   -0.126243    -0.134607   \n",
      "ALL  -0.377936    -0.210822   0.723277      0.042915   -0.042336    -0.184456   \n",
      "LLA  -0.325802    -0.160612   0.716421      0.140574   -0.105720    -0.169147   \n",
      "SSS   0.175785    -0.103099   0.445012     -0.116389   -0.366477    -0.205607   \n",
      "LAL  -0.347601    -0.276936   0.759509      0.073802    0.040129    -0.204613   \n",
      "EAL  -0.476662    -0.229644   0.596552     -0.157708   -0.179475    -0.120480   \n",
      "AAG  -0.460274    -0.385322   0.771879      0.123707   -0.182858    -0.174014   \n",
      "\n",
      "     -0.5336818  -0.11842774  -0.29594347  0.20798174  ...  0.10346055  \\\n",
      "AAA                                                    ...               \n",
      "ALA   -0.418640    -0.108126    -0.080837    0.071797  ...    0.049553   \n",
      "LLL   -0.390646    -0.066851    -0.001190    0.004624  ...    0.088149   \n",
      "LAA   -0.382097    -0.067657    -0.075877    0.052444  ...   -0.055329   \n",
      "AAL   -0.416561    -0.080616    -0.120227    0.101790  ...    0.029709   \n",
      "ALL   -0.404717    -0.124001     0.019849    0.099604  ...    0.068476   \n",
      "LLA   -0.467110    -0.114021    -0.017606    0.069313  ...   -0.003497   \n",
      "SSS   -0.413370    -0.172332    -0.053659   -0.006575  ...    0.038888   \n",
      "LAL   -0.410828    -0.027634     0.006813    0.094726  ...   -0.026143   \n",
      "EAL   -0.296757    -0.170280    -0.008014    0.068219  ...    0.067036   \n",
      "AAG   -0.386539    -0.062943    -0.118821    0.030014  ...    0.081578   \n",
      "\n",
      "     0.08766685  -0.002761118  -0.27165273  0.05406976  0.077483445  \\\n",
      "AAA                                                                   \n",
      "ALA    0.059046     -0.010251    -0.172349    0.029979    -0.031024   \n",
      "LLL    0.072813     -0.123380    -0.199892    0.042284    -0.053610   \n",
      "LAA    0.084730      0.001758    -0.199165    0.004857    -0.003792   \n",
      "AAL    0.035406     -0.047870    -0.132200    0.046381    -0.019915   \n",
      "ALL    0.131889     -0.057058    -0.164135    0.003458    -0.097322   \n",
      "LLA    0.108235     -0.090846    -0.110949    0.078946    -0.042742   \n",
      "SSS    0.087968     -0.238242    -0.545965   -0.068023     0.111635   \n",
      "LAL    0.139871     -0.144142    -0.153625   -0.012268     0.016268   \n",
      "EAL    0.167827      0.069834    -0.188456    0.162565    -0.124384   \n",
      "AAG    0.104950     -0.007324    -0.271840    0.052439     0.065907   \n",
      "\n",
      "     -0.11654436  -0.029171592  0.010106765  0.20724945  \n",
      "AAA                                                      \n",
      "ALA    -0.102782      0.114467     0.055506    0.167345  \n",
      "LLL    -0.057742      0.031487     0.116428    0.052871  \n",
      "LAA    -0.077541      0.029376     0.034310    0.229972  \n",
      "AAL    -0.063717      0.073418     0.010455    0.117893  \n",
      "ALL    -0.000164      0.102995     0.075068    0.094654  \n",
      "LLA    -0.049499      0.071640     0.017914    0.075820  \n",
      "SSS     0.001057     -0.107623     0.237846    0.259407  \n",
      "LAL    -0.041680      0.064420     0.007695    0.096143  \n",
      "EAL    -0.018320      0.114497    -0.036499    0.117207  \n",
      "AAG    -0.122258      0.002264     0.090544    0.104717  \n",
      "\n",
      "[10 rows x 100 columns]\n",
      "Size of the DataFrame: (10410, 100)\n",
      "Unique Values: 10410\n",
      "\n",
      "Example Protein Sequence:\n",
      "\n",
      "MTDTTLPPGDGPVDRIEPVDIQQEMQRSYIDYAMSVIVGRALPEVRDGLKPVHRRVLYAMYDSGFRPDRSHAKSARSVAETMGNYHPHGDASIYDTLVRMAQPWSLRYPLVDGQGNFGSPGNDPPAAMRYCVTGDALVRLPFGQAVRIGDIVPGARSNSDNPVELKVLDRHGNPVVADRLFHSGDHQTYTVRTAEGYEVTGTANHPLLCLVDLGGVPTLLWKLIEEVRPDDYVVLQRTPPVEFGPADWHDAMEALLLGAFISEGFVSEFRAGFNNLDRDYFNTVVSAYDAVVGGRRYVSQRTIASGSLLNELDIHNLSALKNTRLRELCGQRSADKSVPDWLWHSHAAVKRAFLQALFEGDGSCSALPRNTIQISYSTRSKQLAIDVQQMLLELGVVSKRYRHTVGEYKVVITNRAEAENFAGQIGFGGAKQDKLTGILSSLPPCAGRDNDHVPGLAAFIRSHCDSRWVEKEWLRKHNIDRPSRWRRDASEILSRIANPDVRAIATDLTDGRFYYAKVASVTEAGVQPVYSLRVDTDDHAFLTNGFVSHNTEARLTPLAMEMLREIDEETVDFIPNYDGRVQEPTVLPSRFPNLLANGSGGIAVGMATNIPPHNLRELAEAVFWCLENYDADEEATLTAVMERVKGPDFPTSGLIVGSQGISDAYKTGRGSIRMRGVVEVEEDSRGRTSLVITELPYQVNHDNFITSIAEQVRDGRLAGISNIEDQSSDRVGLRIVVEIKRDAVAKVVLNNLYKHTQLQTSFGANMLSIVDGVPRTLRLDQMIRYYVEHQLDVIVRRTTYRLRKANERAHILRGLVKALDALDEVIALIRASETVDIARQGLIELLDIDEIQAQAILDMQLRRLAALERQRIVDDLAKIEAEIADLEDILAKPERQRGIVRDELGEIVEKHGDDRRTRIIAADGDVSDEDLIAREDVVVTITETGYAKRTKTDLYRSQKRGGKGVQGAGLKQDDIVRHFFVCSTHDWILFFTTQGRVYRAKAYELPEALRTARGQHVANLLAFQPEERIAQVIQIKSYEDAPYLVLATQNGLVKKSKLTDFDSNRSGGIVAVNLRDGDELVGAVLCSAEEDLLLVSANGQSIRFSATDEALRPMGRATSGVQGMRFNTDDRLLSLNVVREGTYLLVATAGGYAKRTGIEEYPVQGRGGKGVLTIMYDRRRGRLVGALIVDDESELYAITSVGGVIRTTAKQVRKAGRQTKGVRLMNLGEGDTLLAIARNAEESADAEVEGEEAGS\n"
     ]
    }
   ],
   "source": [
    "data_path = \"3-gram-final.csv\"\n",
    "sequence_path = \"DNA_Gyrase_UniProt_CLEANED.fasta\"\n",
    "\n",
    "model_df = pd.read_csv(data_path, index_col=0, delimiter=',')\n",
    "print(\"Model Head:\")\n",
    "print(model_df.head(10))\n",
    "print(\"Size of the DataFrame:\", model_df.shape)\n",
    "unique_values = model_df.index.nunique()\n",
    "print(f\"Unique Values: {unique_values}\")\n",
    "sequences = SeqIO.to_dict(SeqIO.parse(sequence_path, \"fasta\"))\n",
    "print(\"\\nExample Protein Sequence:\\n\")\n",
    "print(str(list(sequences.values())[0].seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cf8de8",
   "metadata": {},
   "source": [
    "## Standardization and Vectorization\n",
    "* generate_kmers - Generate overlapping k-mers of given length from a Protein sequence\n",
    "* prot_vectorization - Vectorize Protein k-mers using a model loaded from a CSV file\n",
    "* standardize_vectors - Standardize the feature vectors\n",
    "* vectorize_sequences - Vectorize sequences based on a model DataFrame\n",
    "* check_kmers_in_model - Find k-mers in model dataframe\n",
    "* get_standardize_vector_sequence - Returns Standardize vectorized sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63580ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_kmers(prot_sequence, k=3):\n",
    "    \"\"\"Generate overlapping k-mers from a Protein sequence.\"\"\"\n",
    "    return [prot_sequence[i:i+k] for i in range(len(prot_sequence) - k + 1)]\n",
    "\n",
    "def prot_vectorization(kmers, model_df, dimensions):\n",
    "    \"\"\"Vectorize Protein k-mers using a model loaded from a CSV file.\"\"\"\n",
    "    vector = np.zeros((len(kmers), dimensions))\n",
    "    for i, kmer in enumerate(kmers):\n",
    "        if kmer in model_df.index:\n",
    "            vector[i] = model_df.loc[kmer].values\n",
    "    return np.mean(vector, axis=0)\n",
    "\n",
    "def standardize_vectors(vectors):\n",
    "    \"\"\"Standardize the feature vectors.\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    standardized_vectors = scaler.fit_transform(vectors)\n",
    "    return standardized_vectors\n",
    "\n",
    "def vectorize_sequences(sequences, model_df, k):\n",
    "    \"\"\"Vectorize sequences based on a model DataFrame.\"\"\"\n",
    "    vectorized_seqs = []\n",
    "    seq_ids = []\n",
    "    \n",
    "    for seq_id, sequence in sequences.items():\n",
    "        kmers = generate_kmers(str(sequence.seq), k)\n",
    "        dimensions = model_df.shape[1]\n",
    "        vec = prot_vectorization(kmers, model_df, dimensions)\n",
    "        vectorized_seqs.append(vec)\n",
    "        seq_ids.append(seq_id)\n",
    "    \n",
    "    return vectorized_seqs, seq_ids\n",
    "\n",
    "def check_kmers_in_model(kmers, model_df):\n",
    "    \"\"\"Check if k-mers are in the model DataFrame.\"\"\"\n",
    "    for kmer in kmers:\n",
    "        if kmer in model_df.index:\n",
    "            print(f\"{kmer} found in model\")\n",
    "        else:\n",
    "            print(f\"{kmer} NOT found in model\")\n",
    "            \n",
    "def get_standardize_vector_sequence(vectorized_seqs):\n",
    "    \"\"\"Standardize vectorized sequences.\"\"\"\n",
    "    vectorized_seqs_np = np.array(vectorized_seqs)\n",
    "    if len(vectorized_seqs_np.shape) == 1:\n",
    "        vectorized_seqs_np = np.stack(vectorized_seqs_np, axis=0)\n",
    "    \n",
    "    standardized_vectors = standardize_vectors(vectorized_seqs_np)\n",
    "    return standardized_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e7f0c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reads in clustalW MSA and finds conserved regions and generates weights for the embeddings.\n",
    "from Bio import AlignIO\n",
    "\n",
    "def get_conservation_weights(alignment_file):\n",
    "    \"\"\"\n",
    "    Parses the Clustal alignment file to determine conservation weights.\n",
    "    \"\"\"\n",
    "    alignment = AlignIO.read(alignment_file, \"clustal\")\n",
    "    conservation_line = alignment[-1].seq  # Assuming conservation info is in the last sequence\n",
    "    \n",
    "    weights = {}\n",
    "    for i, symbol in enumerate(conservation_line):\n",
    "        if symbol == '*':  # Fully conserved\n",
    "            weights[i] = 3\n",
    "        elif symbol == ':':  # Strongly conserved\n",
    "            weights[i] = 2\n",
    "        elif symbol == '.':  # Weaker conserved\n",
    "            weights[i] = 1\n",
    "    \n",
    "    return weights\n",
    "\n",
    "# Example usage\n",
    "alignment_file = \"DNA_Gyrase_MSA.txt\"\n",
    "weights = get_conservation_weights(alignment_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d19a0fc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorized_seqs, seq_ids = vectorize_sequences(sequences, model_df, k=3)\n",
    "\n",
    "if len(vectorized_seqs) == 0 or vectorized_seqs[0].size == 0:\n",
    "    print(\"No sequences were vectorized. Check the vectorization process.\")\n",
    "else:\n",
    "    # Standardize vectors\n",
    "    standardized_vectors = get_standardize_vector_sequence(vectorized_seqs)\n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame(standardized_vectors, index=seq_ids)\n",
    "    df.to_csv('gyrase_vectorized1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed703745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_13908\\426250307.py\", line 20, in <module>\n",
      "    vec = weighted_prot_vectorization(kmers, model_df, dimensions, weights)  # Now using the modified function\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_13908\\426250307.py\", line -1, in weighted_prot_vectorization\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# weighted embeddings for active site or regions which are conserved and thus play vital roles in the sequence function of DNA Gyrase\n",
    "def weighted_prot_vectorization(kmers, model_df, dimensions, weights):\n",
    "    \"\"\"\n",
    "    Vectorize Protein k-mers using a model loaded from a CSV file, applying conservation weights.\n",
    "    \"\"\"\n",
    "    vector = np.zeros((len(kmers), dimensions))\n",
    "    for i, kmer in enumerate(kmers):\n",
    "        if kmer in model_df.index:\n",
    "            # Apply conservation weight if the position is conserved; otherwise, use default weight of 1\n",
    "            weight = np.mean([weights.get(pos, 1) for pos in range(i, i + len(kmer))])\n",
    "            vector[i] = model_df.loc[kmer].values * weight\n",
    "    return np.mean(vector, axis=0)\n",
    "\n",
    "# Integration with existing code\n",
    "vectorized_seqs, seq_ids = [], []\n",
    "\n",
    "for seq_id, sequence in sequences.items():\n",
    "    kmers = generate_kmers(str(sequence.seq), k=3)\n",
    "    dimensions = model_df.shape[1]\n",
    "    vec = weighted_prot_vectorization(kmers, model_df, dimensions, weights)  # Now using the modified function\n",
    "    vectorized_seqs.append(vec)\n",
    "    seq_ids.append(seq_id)\n",
    "\n",
    "# Proceed with standardization and further analysis as before\n",
    "standardized_vectors = get_standardize_vector_sequence(vectorized_seqs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c826d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized = pd.read_csv(\"gyrase_vectorized1.csv\", index_col=0, delimiter=',')\n",
    "print(standardized.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8716fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash/env python \n",
    "import sys,json,os \n",
    "import matplotlib.pyplot as plt \n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis \n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd \n",
    "kmers = [] \n",
    "# Parse prot file\n",
    "with open(\"gyrase_vectorized1.csv\",\"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip() \n",
    "        cols = line.split(',') \n",
    "        kmer = cols[0]\n",
    "        if 'X' in kmer or 'Z' in kmer or 'B' in kmer or '<' in kmer:\n",
    "            continue \n",
    "        kmers.append(kmer)  \n",
    "\n",
    "\n",
    "\n",
    "X = pd.read_csv(\"gyrase_vectorized1.csv\",header=None)\n",
    "print(X)\n",
    "\n",
    "kmer_names = X.iloc[:,0].values\n",
    "X = X.iloc[:, 1:len(X.columns)-1]\n",
    "X = X.values \n",
    "\n",
    "# t-distributed Stochastic Neighbor Embedding. (Like PCA but based on similarity not covariance)\n",
    "print('-- Fitting TSNE')\n",
    "pca = TSNE(n_components=2)\n",
    "X_trans = pca.fit_transform(X)\n",
    "\n",
    "\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_trans.shape)\n",
    "\n",
    "plt.scatter(X_trans[:, 1], X_trans[:, 0], alpha=0.5,s=4)\n",
    "df = pd.DataFrame(X_trans,index=kmer_names)\n",
    "df.to_csv(\"3-gram-dbtx.model\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9619016",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'DNA Gyrase_Uniprot.fasta'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Tokenize your protein sequences\u001b[39;00m\n\u001b[0;32m     18\u001b[0m sequence_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDNA Gyrase_Uniprot.fasta\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 19\u001b[0m tokenized_sequences \u001b[38;5;241m=\u001b[39m tokenize_sequences(sequence_path)\n",
      "Cell \u001b[1;32mIn[37], line 10\u001b[0m, in \u001b[0;36mtokenize_sequences\u001b[1;34m(sequence_path)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_sequences\u001b[39m(sequence_path):\n\u001b[0;32m      9\u001b[0m     tokenized_sequences \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m SeqIO\u001b[38;5;241m.\u001b[39mparse(sequence_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfasta\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     11\u001b[0m         sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(record\u001b[38;5;241m.\u001b[39mseq)\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;66;03m# Tokenize sequence\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\Bio\\SeqIO\\__init__.py:613\u001b[0m, in \u001b[0;36mparse\u001b[1;34m(handle, format, alphabet)\u001b[0m\n\u001b[0;32m    611\u001b[0m iterator_generator \u001b[38;5;241m=\u001b[39m _FormatToIterator\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iterator_generator:\n\u001b[1;32m--> 613\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m iterator_generator(handle)\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01min\u001b[39;00m AlignIO\u001b[38;5;241m.\u001b[39m_FormatToIterator:\n\u001b[0;32m    615\u001b[0m     \u001b[38;5;66;03m# Use Bio.AlignIO to read in the alignments\u001b[39;00m\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (r \u001b[38;5;28;01mfor\u001b[39;00m alignment \u001b[38;5;129;01min\u001b[39;00m AlignIO\u001b[38;5;241m.\u001b[39mparse(handle, \u001b[38;5;28mformat\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m alignment)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\Bio\\SeqIO\\FastaIO.py:190\u001b[0m, in \u001b[0;36mFastaIterator.__init__\u001b[1;34m(self, source, alphabet)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m alphabet \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe alphabet argument is no longer supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 190\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(source, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m, fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFasta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\Bio\\SeqIO\\Interfaces.py:58\u001b[0m, in \u001b[0;36mSequenceIterator.__init__\u001b[1;34m(self, source, alphabet, mode, fmt)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe alphabet argument is no longer supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, _PathLikeTypes):\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(source, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m mode)\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_close_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'DNA Gyrase_Uniprot.fasta'"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from Bio import SeqIO\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "\n",
    "# Function to tokenize a sequence\n",
    "def tokenize_sequences(sequence_path):\n",
    "    tokenized_sequences = []\n",
    "    for record in SeqIO.parse(sequence_path, \"fasta\"):\n",
    "        sequence = str(record.seq)\n",
    "        # Tokenize sequence\n",
    "        encoded_sequence = tokenizer.encode(sequence, add_special_tokens=True, truncation=True, max_length=512)\n",
    "        tokenized_sequences.append(encoded_sequence)\n",
    "    return tokenized_sequences\n",
    "\n",
    "# Tokenize your protein sequences\n",
    "sequence_path = \"DNA Gyrase_Uniprot.fasta\"\n",
    "tokenized_sequences = tokenize_sequences(sequence_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce46a03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the pre-computed vectors\n",
    "vectors_path = \"gyrase_vectorized1.csv\"\n",
    "vectors_df = pd.read_csv(vectors_path, index_col=0)\n",
    "\n",
    "# Convert DataFrame to a tensor or array for use with PyTorch or other libraries\n",
    "import torch\n",
    "vectors_tensor = torch.tensor(vectors_df.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae1b43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, tokenized_sequences, vectors_tensor):\n",
    "        self.tokenized_sequences = tokenized_sequences\n",
    "        self.vectors_tensor = vectors_tensor\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            \"input_ids\": torch.tensor(self.tokenized_sequences[idx], dtype=torch.long),\n",
    "            \"vectors\": self.vectors_tensor[idx]\n",
    "        }\n",
    "        return item\n",
    "\n",
    "\n",
    "dataset = ProteinDataset(tokenized_sequences, vectors_tensor)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8387d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Initialize ProtBert\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "protbert = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, bert_model, custom_embedding_dim, hidden_dim, output_dim):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.bert_model = bert_model\n",
    "        self.concat_layer = nn.Linear(bert_model.config.hidden_size + custom_embedding_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.classifier = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, custom_embeddings):\n",
    "        # Get embeddings from ProtBert\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            sequence_output = outputs.last_hidden_state[:, 0, :]  # Using the [CLS] token\n",
    "        \n",
    "        # Concatenate ProtBert embeddings with custom embeddings\n",
    "        combined_features = torch.cat((sequence_output, custom_embeddings), dim=1)\n",
    "        \n",
    "        # Pass through additional layers\n",
    "        x = self.concat_layer(combined_features)\n",
    "        x = self.relu(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e64fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d77e60f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
